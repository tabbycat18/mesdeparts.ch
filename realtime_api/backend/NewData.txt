Imports from your Mac terminal (psql)

Important: don’t wrap \copy in BEGIN/COMMIT (that’s what caused the “idle-in-transaction timeout” pain).
Reminder: the RT stack only uses GTFS stop_id values from the feed (including platform/Parent suffixes like `Parent8501120`). Do not try to import or look up transport.opendata.ch IDs here.
	1.	Connect:

psql "$NEON_URL"

	2.	In psql, make it fail-fast:

\set ON_ERROR_STOP on
\timing on

	3.	Run imports (adjust filenames if yours differ):

  TRUNCATE public.stop_times,
         public.trips,
         public.calendar_dates,
         public.calendar,
         public.routes,
         public.stops,
         public.agencies
RESTART IDENTITY;

\copy public.agencies (agency_id, agency_name, agency_url, agency_timezone, agency_lang, agency_phone)
FROM '/Users/mattiapastore/Documents/VSC/mesdeparts.ch/realtime_api/data/gtfs-static-local/agency.csv'
WITH (FORMAT csv, HEADER true, QUOTE '"', ESCAPE '"', NULL '');

\copy public.stops (stop_id, stop_name, stop_lat, stop_lon, location_type, parent_station, platform_code)
FROM '/Users/mattiapastore/Documents/VSC/mesdeparts.ch/realtime_api/data/gtfs-static-local/stops.csv'
WITH (FORMAT csv, HEADER true, QUOTE '"', ESCAPE '"', NULL '');

UPDATE public.stops
SET original_stop_id = stop_id
WHERE original_stop_id IS NULL;

\copy public.routes (route_id, agency_id, route_short_name, route_long_name, route_desc, route_type)
FROM '/Users/mattiapastore/Documents/VSC/mesdeparts.ch/realtime_api/data/gtfs-static-local/routes.csv'
WITH (FORMAT csv, HEADER true, QUOTE '"', ESCAPE '"', NULL '');

\copy public.calendar (service_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday, start_date, end_date)
FROM '/Users/mattiapastore/Documents/VSC/mesdeparts.ch/realtime_api/data/gtfs-static-local/calendar.csv'
WITH (FORMAT csv, HEADER true, QUOTE '"', ESCAPE '"', NULL '');

\copy public.calendar_dates (service_id, date, exception_type)
FROM '/Users/mattiapastore/Documents/VSC/mesdeparts.ch/realtime_api/data/gtfs-static-local/calendar_dates.csv'
WITH (FORMAT csv, HEADER true, QUOTE '"', ESCAPE '"', NULL '');

Trips import (do this safely)

Because trips.csv columns vary by feed, do this first in your Mac terminal to get the exact header:

head -n 1 /Users/mattiapastore/Documents/VSC/mesdeparts.ch/realtime_api/data/gtfs-static-local/trips.csv

Then use that exact order in the \copy (...) list.

Example (based on what you were using):

\copy public.trips (route_id,service_id,trip_id,trip_headsign,trip_short_name,direction_id,block_id,original_trip_id,hints)
FROM '/Users/mattiapastore/Documents/VSC/mesdeparts.ch/realtime_api/data/gtfs-static-local/trips.csv'
WITH (FORMAT csv, HEADER true, QUOTE '"', ESCAPE '"', NULL '');

Stop times import

\copy public.stop_times (trip_id, arrival_time, departure_time, stop_id, stop_sequence, pickup_type, drop_off_type)
FROM '/Users/mattiapastore/Documents/VSC/mesdeparts.ch/realtime_api/data/gtfs-static-local/stop_times.csv'
WITH (FORMAT csv, HEADER true, QUOTE '"', ESCAPE '"', NULL '');


⸻

Post-import “seconds fill” (the step you already did)

UPDATE public.stop_times
SET
  arrival_time_seconds = CASE
    WHEN arrival_time ~ '^[0-9]{1,3}:[0-9]{2}(:[0-9]{2})?$' THEN
      split_part(arrival_time, ':', 1)::int * 3600 +
      split_part(arrival_time, ':', 2)::int * 60 +
      COALESCE(NULLIF(split_part(arrival_time, ':', 3), '')::int, 0)
    ELSE NULL
  END,
  departure_time_seconds = CASE
    WHEN departure_time ~ '^[0-9]{1,3}:[0-9]{2}(:[0-9]{2})?$' THEN
      split_part(departure_time, ':', 1)::int * 3600 +
      split_part(departure_time, ':', 2)::int * 60 +
      COALESCE(NULLIF(split_part(departure_time, ':', 3), '')::int, 0)
    ELSE NULL
  END;

CREATE INDEX IF NOT EXISTS stop_times_stop_departure_idx
  ON public.stop_times (stop_id, departure_time_seconds);

(Optional but recommended right after big imports)

ANALYZE public.stop_times;
ANALYZE public.stops;
ANALYZE public.trips;


⸻

Integrity checks (the “trip_id mismatch” detector)

SELECT 'agencies' AS t, COUNT(*) FROM public.agencies
UNION ALL SELECT 'stops', COUNT(*) FROM public.stops
UNION ALL SELECT 'routes', COUNT(*) FROM public.routes
UNION ALL SELECT 'calendar', COUNT(*) FROM public.calendar
UNION ALL SELECT 'calendar_dates', COUNT(*) FROM public.calendar_dates
UNION ALL SELECT 'trips', COUNT(*) FROM public.trips
UNION ALL SELECT 'stop_times', COUNT(*) FROM public.stop_times;

-- Stop_times referencing a trip_id that doesn’t exist in trips
SELECT COUNT(*) AS stop_times_missing_trip
FROM public.stop_times st
LEFT JOIN public.trips t ON t.trip_id = st.trip_id
WHERE t.trip_id IS NULL;

-- Trips that have no stop_times (not always an error, but useful)
SELECT COUNT(*) AS trips_missing_stop_times
FROM public.trips t
LEFT JOIN public.stop_times st ON st.trip_id = t.trip_id
WHERE st.trip_id IS NULL;


⸻

Rebuild / refresh search_stops (yes, do it)

The view is created by sql/legacy/schema_gtfs.sql (legacy reference). After each import:

REFRESH MATERIALIZED VIEW public.search_stops;


⸻

“New timetable starts in 24h” — do you need to reimport?

[Inference] Usually it’s not a nightmare anymore: if the new GTFS feed has the same columns, you can do a TRUNCATE + reimport + seconds update + refresh search_stops.

Before doing anything, check whether your current GTFS already covers tomorrow:

SELECT
  MIN(to_date(start_date,'YYYYMMDD')) AS min_start,
  MAX(to_date(end_date,'YYYYMMDD'))   AS max_end
FROM public.calendar;

Yearly update (fast path)

If schema is unchanged, do this instead of dropping tables:

TRUNCATE public.stop_times,
         public.trips,
         public.calendar_dates,
         public.calendar,
         public.routes,
         public.stops,
         public.agencies
RESTART IDENTITY;

Then rerun the \copy imports, rerun the “seconds fill” UPDATE, and REFRESH MATERIALIZED VIEW public.search_stops;.
